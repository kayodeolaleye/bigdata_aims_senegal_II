{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model Evaluation and Selection</h1>\n",
    "\n",
    "<p>In this assignment we will introduce some techniques to evaluate the quality of a method and how to select good parameter values.</p>\n",
    "\n",
    "<p>We will be using the scikit built-in breast_cancer data set. It is binary classification problem where breast masses are classified as malignin (equal 0) or benign (equal 1).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are  30 features in the datasets namely: \n",
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "dataset = load_breast_cancer()\n",
    "\n",
    "data = dataset.data\n",
    "target = dataset.target\n",
    "\n",
    "###Find how many features we have and their names\n",
    "features = dataset.feature_names\n",
    "print \"there are \",dataset.feature_names.size, 'features in the datasets namely: ' \n",
    "print dataset.feature_names\n",
    "\n",
    "#The columns 10 to 19 are measurements errors and we can drop them without affecting much the work done here\n",
    "###Remove the columns 10 to 19 in the data\n",
    "\n",
    "index = [10, 19]\n",
    "features_sub = np.delete(features, index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Example on a Single Decision Tree</h2>\n",
    "\n",
    "<p>In this section we will introduce evaluation and paramtere selection techniques on a single decision tree.</p>\n",
    "\n",
    "<h4>Simple Evaluation</h4>\n",
    "<p>Evaluating the accuracy of a method can naively be done by splitting the data set in a training set and a test set.\n",
    "We train our classifier on the training set (obviously) and we evaluate the accuracy on the test set.<br>\n",
    "In scikit this is easily done by using the <i>.score()</i> functions of the classifier.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "###Split the data in train and test sets and the target in train_target and test_target (ratio 70%-30%)\n",
    "## Hint : by using the keyword \"random_state=0\" when you call train_test_split\n",
    "##        you make sure that the splits are the same for both data and target\n",
    "\n",
    "train, test, train_target, test_target = train_test_split(data, target, test_size=0.3, random_state=0)\n",
    "\n",
    "###Import a decision tree and train it on the training set with the default settings\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(train, train_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91812865497076024"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Compute the accuracy on the test set\n",
    "pred_test = dtc.predict(test)\n",
    "dtc.score(test, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The accuracy is simply giving you the amount of samples that have been correctly classified<br>\n",
    "Other methods to measure the quality of the classifier are available. For instance one can use the F1 score. F1 score use the <i>precision</i> and <i>recall</i> (see https://en.wikipedia.org/wiki/Precision_and_recall) to evaluate the quality of a classification.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93333333333333335"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(test_target,pred_test,average='binary')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>It is also possible to have the detail of precision and recall for both classes :</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "  malignant       0.86      0.94      0.89        63\n",
      "     benign       0.96      0.91      0.93       108\n",
      "\n",
      "avg / total       0.92      0.92      0.92       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print classification_report(test_target, pred_test, target_names=['malignant', 'benign'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We started this study by doing a random separation for the train/test sets. Actually all scores of tests performed so far depend on this separation.</p>\n",
    "\n",
    "<h4> <u>QUESTION 1 :</u> Explain why all scores are specific to our first sets split.</h4>\n",
    "<p><i>The function, <em style=\"color:red\">train_test_split</em> splits data into subsets randomly (using pseudo-random number generator).</i></p>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<h4>Cross Validation</h4>\n",
    "\n",
    "<p>Another way to evaluate the accuracy of a decision tree (and any method in general) is to use a cross validation technique :\n",
    "Basically it consists in first dividing the data set in <i>k</i> sets named <i>folds</i>, then train the classifier on <i>k-1</i> folds and evaluate the accuracy on the remaining fold.</p>\n",
    "<p>In scikit this can be done by using the <i>cross_val_score</i> function</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "###use the cross_val_score function on the whole dataset\n",
    "cvs = \n",
    "\n",
    "#When calling the cross_val_score it returns one score per fold\n",
    "#As by default the function uses a three-fold separation you have three value\n",
    "###Compute and print the mean and the standard deviation of the cross_val_score function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Several techniques exist to divide the data set in folds (see http://scikit-learn.org/stable/modules/cross_validation.html for more details).</p>\n",
    "<p>Nonetheless, it is worth mentioning another technique : the ShuffleSplit. This technique generates a pre-defined number of independent train/test dataset splits. Samples are first shuffled and then split into a pair of train and test sets.</p>\n",
    "<p>This can be implemented as follow :</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import ShuffleSplit\n",
    "cv_ss = ShuffleSplit(data.shape[0],n_iter=5,test_size=0.4,random_state=0)\n",
    "\n",
    "###Use again the cross_val_score function and set \"cv=cv_ss\"\n",
    "\n",
    "###Compute again the mean and the standard deviation of the output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> <u>QUESTION 2 :</u> Are results very different?</h4>\n",
    "<p><i>Type your answer here</i></p>\n",
    "\n",
    "<h3>Finding Optimal Parameters</h3>\n",
    "\n",
    "<p>In the previous section we have use the default settings for our classifiers but this is usually not necessarily the most optimal choice.</p>\n",
    "<p>In this section we will introduce tools to find good parameters value.</p>\n",
    "\n",
    "<h4>Grid Search - a brute force approach</h4>\n",
    "<p>A decision tree has several parameters we can change to optimize the classification. Scikit offers the possibility to investigate several parameters using <i>GridSearchCV</i>. You simply need to define a \"parameter grid\" (a dictionary in python) that defines the parameters values you want to try and feed it to a GridSearchCV object :</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "###Example of a parameter grid dictionary, run it once and then include more parameters in p_grid\n",
    "p_grid=dict({'criterion':['entropy','gini']})\n",
    "\n",
    "#grd = GridSearchCV(<classifier>,cv=3,param_grid=<dictionary of parameters to investigate>)\n",
    "#grd.fit(<training set>,<train set targets>)\n",
    "grd = GridSearchCV(dtr,cv=3,param_grid=param_grid)\n",
    "grd.fit(train,train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> <u>QUESTION 2 :</u> What does the \"CV\" at the end of GridSearchCV stands for? What is it telling you?</h4>\n",
    "<p><i>Type your answer here</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#You can ask for the best parameters found by running the following command\n",
    "grd.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#And creat directly a new classifier with the optimal parameters by running\n",
    "new_dtr = grd.best_estimator_\n",
    "\n",
    "###Check the \"new_dtr\" parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###Now train the new classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###Compute its accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###print the classification report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Learning and Validation Curves</h4>\n",
    "<p> Scikit provides additional tools to tune our algorithm.<br>\n",
    "One useful tool is the learning curve. It gives the cross-validated training and test scores for different training sets sizes.\n",
    "We can use it on the previously defined new classifier \"new_dtr\" :</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.learning_curve import learning_curve\n",
    "\n",
    "###Compute the learning curve\n",
    "#<number of elements in train>,<train score>,<test score> = learning_curve(<classifier>,<data>,<targets>,train_sizes=<liste of training sizes>,cv=3)\n",
    "#The list of training sizes can be absolute numbers or amount if between (0,1]\n",
    "\n",
    "###Visualize the learning curve (don't forget labels, title,legend,etc)\n",
    "\n",
    "#You should see why I chose a 70%-30% ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> <u>QUESTION 3 :</u> Why isn't the training score equal to one?</h4>\n",
    "<p><i>Type your answer here</i></p>\n",
    "\n",
    "<p>A second tool meant to investigate a specific parameter influence on scores is the <i>validation curve</i>. It is basically like a gridsearch with a single parameter. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.learning_curve import validation_curve\n",
    "\n",
    "#Just an example\n",
    "train_score,test_score = validation_curve(new_dtr,data,target,param_name='max_depth',param_range=np.arange(2,8),cv=3)\n",
    "\n",
    "###Plot the validation curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Application - Evaluating the Random Forest classifier and the SVC</h3>\n",
    "\n",
    "<p>In the following you will apply the evaluation and optimization tools to compare the Random Forest technique and the SVC technique.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###Import the \"RandomForestClassifier\" classifier\n",
    "\n",
    "###Train it on the training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###Import the \"SVC\" classifier\n",
    "\n",
    "###Train it on the training set  (don't forget to scale it!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Before evaluating the performance of both classifiers we will first determine the best values for their parameters</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###Using the grid search optimize the Random Forest Classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###Create a new Random Forest Classifier using the best parameters found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###Using the grid search optimize the SVC Classifier\n",
    "#(!don't forget to scale the data!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###Create a new Random Forest Classifier using the best parameters found\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now we have optimized out two classifier we can compare how they perform</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###Compute and the accuracy of both classifiers on the training set using the cross_val_score\n",
    "# (!scale for SVC!)\n",
    "\n",
    "\n",
    "#Print the results (average and std)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> <u>QUESTION 4 :</u> Which classifier gives the best accuracy?</h4>\n",
    "<p><i>Type your answer here</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###Print the Classification report for the Random Forest Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###Do the same for the SVC classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> <u>QUESTION 5 :</u> Analyze the last two classification reports.</h4>\n",
    "<p><i>Type your answer here</i></p>\n",
    "\n",
    "<h4> <u>QUESTION 6 :</u> Recall the classification report from the optimized decision tree to conclude on the best algorithm to chose to efficiently detect malignant masses.</h4>\n",
    "<p><i>Type your answer here</i></p>\n",
    "\n",
    "<h4> <u>BONUS :</u> Repeat the optimization and evalution procedure with the k-nearest neighbors approach.</h4>\n",
    "<p><i>Type your answer here</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
